{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir=\"./data/\"\n",
    "ticker=\"TSLA\"\n",
    "\n",
    "date_pool=pd.date_range(\"1/1/2019\",\"1/31/2019\",freq=\"B\").strftime(\"%Y%m%d\")\n",
    "date_pool=[d for d in date_pool if os.path.exists(data_dir+\"trades_{}_{}.csv\".format(d,ticker))]\n",
    "\n",
    "train_days=10\n",
    "train_date_list=date_pool[:train_days]\n",
    "test_date_list=date_pool[train_days+1:]\n",
    "time_steps = 50\n",
    "\n",
    "nforward=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.scaler = None\n",
    "    def load_data(self, ticker, date):\n",
    "        df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(date, ticker),index_col=[0],parse_dates=[0])\n",
    "\n",
    "        # Feature Engineering\n",
    "        df[\"direction\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(np.sign)\n",
    "        df[\"pct_change\"]=df[\"trade_px\"].pct_change()\n",
    "\n",
    "        mysign=lambda x: 0 if abs(x)<1e-5 else (1 if x>0 else -1)\n",
    "        df[\"label\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "        # df[\"label\"]=(df[\"trade_px\"].shift(-1)-df[\"trade_px\"]).apply(np.sign) # last version\n",
    "\n",
    "        df.fillna(method=\"ffill\",inplace=True)\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        # print(df.head(10),df.shape)\n",
    "        # print(\"NaN number: \",df.isna().sum().sum())\n",
    "\n",
    "        return df[[\"trade_px\",\"trade_size\",\"pct_change\",\"direction\",\"label\"]].values\n",
    "\n",
    "    def create_dataset(self, ticker=ticker, dates=train_date_list, time_steps = time_steps, input_scaler=None):  \n",
    "        for i,d in enumerate(dates):\n",
    "            datanew = self.load_data(ticker,d)\n",
    "            if i==0:\n",
    "                data=datanew\n",
    "            else:\n",
    "                data=np.vstack((data, datanew))\n",
    "\n",
    "        label=data[:,-1]\n",
    "        data=data[:,:-1]\n",
    "\n",
    "        if input_scaler is None:\n",
    "            scaler=StandardScaler()\n",
    "            data=scaler.fit_transform(data)\n",
    "        else:\n",
    "            data=input_scaler.transform(data)\n",
    "            scaler=input_scaler\n",
    "\n",
    "        x = [data[0 : time_steps]]\n",
    "        y = [label[time_steps-1]]\n",
    "        N=len(data)//time_steps\n",
    "\n",
    "        print(N)\n",
    "        for i in range(1, N):\n",
    "            t = data[i*time_steps: (i + 1)*time_steps]\n",
    "            x = np.vstack((x, [t]))\n",
    "            y.append(label[(i + 1)*time_steps-1])\n",
    "\n",
    "        y=pd.get_dummies(y)\n",
    "        #print(y)\n",
    "\n",
    "        return x,y.values,scaler\n",
    "\n",
    "    def loss_plot(self, history, plot_name = 'Loss'): # type(history) is dict\n",
    "        loss = np.asarray(history['loss'])\n",
    "        val_loss = np.asarray(history['val_loss'])\n",
    "\n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(loss, color = 'darkgrey')\n",
    "        plt.plot(val_loss, color = 'tan')\n",
    "        plt.legend(['loss', 'val_loss'])\n",
    "        # plt.savefig('{}_{}_{}_{}_{}.png'.format(ticker, plot_name, str(n_epochs), str(time_steps), str(batch_size)))\n",
    "    \n",
    "    \n",
    "    def training_data_transform(self, ticker):\n",
    "        # Load train data\n",
    "        x, y, scaler = self.create_dataset(ticker)\n",
    "        self.x, self.y, self.scaler = x, y, scaler\n",
    "        print(\"Finished loading data.\")\n",
    "\n",
    "        with open(\"model/LSTMv2_scaler_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(scaler,f)\n",
    "\n",
    "    def model_training_testing(self, ticker, model, plot = True):\n",
    "        # Model Training pipeline\n",
    "        model_functionalities = Model_Functionalities(model)\n",
    "        \n",
    "        x, y, scaler = self.x, self.y, self.scaler\n",
    "        \n",
    "        if x is None:\n",
    "            print(\"None Training data processed\")\n",
    "            return\n",
    "        \n",
    "        # Build model, in-sample train test\n",
    "        train_history = model_functionalities.train_test(x, y, plot)  \n",
    "        if model_functionalities.model.model_name == \"LSTM\":\n",
    "            if plot == True:\n",
    "                self.loss_plot(train_history.history)\n",
    "\n",
    "        with open(\"model/LSTMv2_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(model,f)\n",
    "\n",
    "        # Out-of-sample test\n",
    "        for test_date in test_date_list:\n",
    "            # create test dateset\n",
    "            x_test, y_test, _ = self.create_dataset(ticker=ticker, dates=[test_date], time_steps = time_steps, input_scaler=scaler)\n",
    "            x_test, y_test = model_functionalities.model.reshape_dataset(x_test, y_test)\n",
    "\n",
    "            # use precious trained model to test\n",
    "            y_test_pred = model_functionalities.predict(x_test)\n",
    "            if model_functionalities.model.model_name == \"LSTM\":\n",
    "                if plot == True:\n",
    "                    model_functionalities.view_accuracy(y_test_pred.argmax(axis=1), y_test.argmax(axis=1))\n",
    "            print(test_date+\" accuracy: \",np.mean(y_test_pred.argmax(axis=1)==y_test.argmax(axis=1)))\n",
    "        return model_functionalities.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15154\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.training_data_transform(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM to predict next up and down\n",
    "## v2: Predict up and downs of the average of $\\mathbf{nforward}=10$ following prices\n",
    "\n",
    "1. Too slow to predict (next trade may happen in millisecond)\n",
    "2. Only classification of up, down and same. No quantitative prediction (can be improved to predict quantity of price movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "activation = \"tanh\"\n",
    "loss = 'categorical_crossentropy'\n",
    "stop_patience=20\n",
    "\n",
    "dpi=200\n",
    "\n",
    "\n",
    "class LSTM_Model():\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model_name = \"LSTM\"\n",
    "        return \n",
    "    \n",
    "    def build(self,  time_steps = time_steps, data_dim = 1, output_dim = 3):\n",
    "        # expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "        # the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "        self.model.add(LSTM(hidden_dim, activation=activation, return_sequences=True, input_shape=(time_steps, data_dim)))\n",
    "        self.model.add(LSTM(hidden_dim, activation=activation, return_sequences=True))\n",
    "        self.model.add(LSTM(hidden_dim, activation=activation))\n",
    "        self.model.add(Dense(output_dim, activation = 'softmax'))\n",
    "        \n",
    "        opt=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        self.model.compile(loss = loss, optimizer=opt, metrics=['accuracy']) \n",
    "        return self.model\n",
    "    \n",
    "    def fit(self, x_train, y_train, batch_size, epochs, validation_data, callbacks):\n",
    "        return self.model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = validation_data, callbacks = callbacks)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def reshape_dataset(self, x, y):\n",
    "        if x is not None:\n",
    "            if len(x.shape) == 2:\n",
    "                x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(y.shape[0],1)\n",
    "        return x, y \n",
    "\n",
    "\n",
    "class Model_Functionalities():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def train_test(self, x, y, plot = False):\n",
    "        \n",
    "        size = len(x)\n",
    "        if size!=len(y):\n",
    "            return None\n",
    "        x = x[: batch_size * (size // batch_size)]\n",
    "        y = y[: batch_size * (size // batch_size)]\n",
    "        \n",
    "        x, y = self.model.reshape_dataset(x, y)\n",
    "\n",
    "        x_train, x_validation, y_train, y_validation= train_test_split(x, y, test_size = 0.1, shuffle = False)\n",
    "        print('train', x_train.shape, y_train.shape)\n",
    "        print('validation', x_validation.shape, y_validation.shape)\n",
    "        \n",
    "        \n",
    "        if self.model.model_name == \"LSTM\":\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=stop_patience, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "            history = self.model.fit(x_train, y_train, batch_size = batch_size, epochs = n_epochs,\n",
    "                                     validation_data=(x_validation, y_validation),callbacks=[early_stopping])\n",
    "        else:\n",
    "            self.model.fit(x_train, y_train)\n",
    "        \n",
    "        self.y_pred = self.model.predict(x_validation)\n",
    "        self.y_validation_true = y_validation\n",
    "        \n",
    "        if plot == True:\n",
    "            self.train_plot = self.view_accuracy(self.predict(x_train).argmax(axis=1), y_train.argmax(axis=1), 'Train')\n",
    "            self.validation_plot = self.view_accuracy(self.predict(x_validation).argmax(axis=1), y_validation.argmax(axis=1), 'Validation')\n",
    "        if self.model.model_name == \"LSTM\":\n",
    "            return history\n",
    "\n",
    "    def predict(self, x_validation):\n",
    "        pred = self.model.predict(x_validation)\n",
    "        return pred\n",
    "    \n",
    "    def view_accuracy(self, y_pred = None, y_true = None, plot_name = 'Test', num=100):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.y_pred.argmax(axis=1)\n",
    "            y_true = self.y_validation_true.argmax(axis=1)\n",
    "        \n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(y_pred[:num], color = 'lightcoral')\n",
    "        plt.plot(y_true[:num], color = 'cornflowerblue', linewidth = 1)\n",
    "        plt.title('{}_{}'.format(ticker, plot_name))\n",
    "        plt.legend(['predict', 'true'])\n",
    "#         if plot_name == 'Test':\n",
    "#             plt.savefig('{}_{}_{}_{}.png'.format(ticker, plot_name, str(time_steps), str(batch_size)))\n",
    "#         else:\n",
    "#             plt.savefig('{}_{}_{}_{}.png'.format(ticker, plot_name, str(time_steps), str(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13593, 50, 4) (13593, 3)\n",
      "validation (1511, 50, 4) (1511, 3)\n",
      "Train on 13593 samples, validate on 1511 samples\n",
      "Epoch 1/100\n",
      "13593/13593 [==============================] - 12s 864us/step - loss: 0.8695 - acc: 0.4786 - val_loss: 0.8538 - val_acc: 0.4924\n",
      "Epoch 2/100\n",
      "13593/13593 [==============================] - 10s 753us/step - loss: 0.8294 - acc: 0.4841 - val_loss: 0.8403 - val_acc: 0.4646\n",
      "Epoch 3/100\n",
      "13593/13593 [==============================] - 10s 763us/step - loss: 0.8074 - acc: 0.5114 - val_loss: 0.8078 - val_acc: 0.5520\n",
      "Epoch 4/100\n",
      "13593/13593 [==============================] - 11s 777us/step - loss: 0.7704 - acc: 0.5687 - val_loss: 0.7815 - val_acc: 0.5659\n",
      "Epoch 5/100\n",
      "13593/13593 [==============================] - 11s 776us/step - loss: 0.7547 - acc: 0.5854 - val_loss: 0.7744 - val_acc: 0.5705\n",
      "Epoch 6/100\n",
      "13593/13593 [==============================] - 11s 773us/step - loss: 0.7484 - acc: 0.5894 - val_loss: 0.7700 - val_acc: 0.5751\n",
      "Epoch 7/100\n",
      "13593/13593 [==============================] - 11s 822us/step - loss: 0.7441 - acc: 0.5928 - val_loss: 0.7671 - val_acc: 0.5665\n",
      "Epoch 8/100\n",
      "13593/13593 [==============================] - 11s 834us/step - loss: 0.7383 - acc: 0.5950 - val_loss: 0.7725 - val_acc: 0.5572\n",
      "Epoch 9/100\n",
      "13593/13593 [==============================] - 11s 810us/step - loss: 0.7363 - acc: 0.5964 - val_loss: 0.7687 - val_acc: 0.5639\n",
      "Epoch 10/100\n",
      "13593/13593 [==============================] - 11s 784us/step - loss: 0.7347 - acc: 0.5937 - val_loss: 0.7707 - val_acc: 0.5572\n",
      "Epoch 11/100\n",
      "13593/13593 [==============================] - 11s 777us/step - loss: 0.7319 - acc: 0.5987 - val_loss: 0.7620 - val_acc: 0.5586\n",
      "Epoch 12/100\n",
      "13593/13593 [==============================] - 11s 807us/step - loss: 0.7318 - acc: 0.5940 - val_loss: 0.7618 - val_acc: 0.5711\n",
      "Epoch 13/100\n",
      "13593/13593 [==============================] - 12s 847us/step - loss: 0.7289 - acc: 0.5986 - val_loss: 0.7649 - val_acc: 0.5665\n",
      "Epoch 14/100\n",
      "13593/13593 [==============================] - 11s 788us/step - loss: 0.7285 - acc: 0.5971 - val_loss: 0.7604 - val_acc: 0.5559\n",
      "Epoch 15/100\n",
      "13593/13593 [==============================] - 12s 866us/step - loss: 0.7284 - acc: 0.5993 - val_loss: 0.7625 - val_acc: 0.5678\n",
      "Epoch 16/100\n",
      "13593/13593 [==============================] - 11s 806us/step - loss: 0.7272 - acc: 0.5974 - val_loss: 0.7610 - val_acc: 0.5751\n",
      "Epoch 17/100\n",
      "13593/13593 [==============================] - 10s 767us/step - loss: 0.7258 - acc: 0.6016 - val_loss: 0.7607 - val_acc: 0.5811\n",
      "Epoch 18/100\n",
      "13593/13593 [==============================] - 11s 775us/step - loss: 0.7247 - acc: 0.5990 - val_loss: 0.7601 - val_acc: 0.5764\n",
      "Epoch 19/100\n",
      "13593/13593 [==============================] - 11s 804us/step - loss: 0.7239 - acc: 0.6021 - val_loss: 0.7674 - val_acc: 0.5771\n",
      "Epoch 20/100\n",
      "13593/13593 [==============================] - 11s 775us/step - loss: 0.7236 - acc: 0.6013 - val_loss: 0.7617 - val_acc: 0.5645\n",
      "Epoch 21/100\n",
      "13593/13593 [==============================] - 10s 758us/step - loss: 0.7217 - acc: 0.6033 - val_loss: 0.7641 - val_acc: 0.5685\n",
      "Epoch 22/100\n",
      "13593/13593 [==============================] - 11s 774us/step - loss: 0.7232 - acc: 0.5996 - val_loss: 0.7649 - val_acc: 0.5659\n",
      "Epoch 23/100\n",
      "13593/13593 [==============================] - 10s 768us/step - loss: 0.7218 - acc: 0.6010 - val_loss: 0.7646 - val_acc: 0.5645\n",
      "Epoch 24/100\n",
      "13593/13593 [==============================] - 10s 770us/step - loss: 0.7202 - acc: 0.6044 - val_loss: 0.7647 - val_acc: 0.5711\n",
      "Epoch 25/100\n",
      "13593/13593 [==============================] - 10s 760us/step - loss: 0.7198 - acc: 0.6029 - val_loss: 0.7622 - val_acc: 0.5672\n",
      "Epoch 26/100\n",
      "13593/13593 [==============================] - 11s 777us/step - loss: 0.7190 - acc: 0.6036 - val_loss: 0.7630 - val_acc: 0.5751\n",
      "Epoch 27/100\n",
      "13593/13593 [==============================] - 10s 762us/step - loss: 0.7190 - acc: 0.6054 - val_loss: 0.7656 - val_acc: 0.5645\n",
      "Epoch 28/100\n",
      "13593/13593 [==============================] - 10s 768us/step - loss: 0.7168 - acc: 0.6069 - val_loss: 0.7654 - val_acc: 0.5559\n",
      "Epoch 29/100\n",
      "13593/13593 [==============================] - 10s 762us/step - loss: 0.7166 - acc: 0.6101 - val_loss: 0.7698 - val_acc: 0.5639\n",
      "Epoch 30/100\n",
      "13593/13593 [==============================] - 11s 791us/step - loss: 0.7146 - acc: 0.6147 - val_loss: 0.7654 - val_acc: 0.5612\n",
      "Epoch 31/100\n",
      "13593/13593 [==============================] - 11s 808us/step - loss: 0.7141 - acc: 0.6129 - val_loss: 0.7677 - val_acc: 0.5685\n",
      "Epoch 32/100\n",
      "13593/13593 [==============================] - 11s 781us/step - loss: 0.7119 - acc: 0.6142 - val_loss: 0.7709 - val_acc: 0.5572\n",
      "Epoch 33/100\n",
      "13593/13593 [==============================] - 11s 780us/step - loss: 0.7101 - acc: 0.6177 - val_loss: 0.7748 - val_acc: 0.5612\n",
      "Epoch 34/100\n",
      "13593/13593 [==============================] - 11s 793us/step - loss: 0.7118 - acc: 0.6158 - val_loss: 0.7762 - val_acc: 0.5698\n",
      "Epoch 35/100\n",
      "13593/13593 [==============================] - 11s 788us/step - loss: 0.7093 - acc: 0.6189 - val_loss: 0.7706 - val_acc: 0.5625\n",
      "Epoch 36/100\n",
      "13593/13593 [==============================] - 11s 780us/step - loss: 0.7072 - acc: 0.6187 - val_loss: 0.7756 - val_acc: 0.5672\n",
      "Epoch 37/100\n",
      "13593/13593 [==============================] - 11s 814us/step - loss: 0.7055 - acc: 0.6244 - val_loss: 0.7745 - val_acc: 0.5579\n",
      "Epoch 38/100\n",
      "13593/13593 [==============================] - 11s 789us/step - loss: 0.7039 - acc: 0.6207 - val_loss: 0.7696 - val_acc: 0.5586\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00038: early stopping\n",
      "808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190117 accuracy:  0.594059405940594\n",
      "5342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190118 accuracy:  0.6330962186447023\n",
      "2570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190122 accuracy:  0.6175097276264592\n",
      "2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190123 accuracy:  0.6304347826086957\n",
      "1759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190124 accuracy:  0.5912450255827174\n",
      "1610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190125 accuracy:  0.5801242236024845\n",
      "1428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190128 accuracy:  0.5889355742296919\n",
      "1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190129 accuracy:  0.5822050290135397\n",
      "2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190130 accuracy:  0.5748613678373382\n",
      "2679\n",
      "20190131 accuracy:  0.610675625233296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LSTM_Model at 0x13b413128>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTM_Model()\n",
    "lstm_model.build(time_steps = time_steps, data_dim = pipeline.x.shape[-1], output_dim = pipeline.y.shape[-1])\n",
    "pipeline.model_training_testing(ticker, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15154, 50, 4) (15154, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.x.shape, pipeline.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self,max_depth=36, n_estimators=100, max_features=0.3, criterion = 'entropy'):\n",
    "        self.model = RandomForestClassifier(n_estimators=n_estimators, criterion = criterion, max_depth=max_depth,max_features=max_features)\n",
    "        self.model_name = \"Random_Forest\"\n",
    "    def fit(self,X,Y):\n",
    "        return self.model.fit(X,Y)\n",
    "    def predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "    def reshape_dataset(self, x, y):\n",
    "        if x is not None:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(y.shape[0],1)\n",
    "        return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13593, 200) (13593, 3)\n",
      "validation (1511, 200) (1511, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.5655940594059405\n",
      "5342\n",
      "20190118 accuracy:  0.6074503931111943\n",
      "2570\n",
      "20190122 accuracy:  0.5883268482490273\n",
      "2944\n",
      "20190123 accuracy:  0.5726902173913043\n",
      "1759\n",
      "20190124 accuracy:  0.5702103467879477\n",
      "1610\n",
      "20190125 accuracy:  0.5875776397515527\n",
      "1428\n",
      "20190128 accuracy:  0.5910364145658263\n",
      "1034\n",
      "20190129 accuracy:  0.5754352030947776\n",
      "2164\n",
      "20190130 accuracy:  0.5651571164510166\n",
      "2679\n",
      "20190131 accuracy:  0.5752146323254946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForest(criterion=None, max_depth=None, max_features=None,\n",
       "             n_estimators=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_model = RandomForest()\n",
    "pipeline.model_training_testing(ticker, random_forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tick Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TICK FACTOR\n",
    "# only update if it's a trade\n",
    "# if message_type == 't':\n",
    "#     # calc the tick\n",
    "#     this_tick = np.sign(last_price - prev_price)\n",
    "#     if this_tick == 0:\n",
    "#         this_tick = prev_tick\n",
    "\n",
    "#     # now calc the tick\n",
    "#     if tick_factor == 0:\n",
    "#         tick_factor = this_tick\n",
    "#     else:\n",
    "#         tick_factor = (tick_ema_alpha * this_tick) + (1 - tick_ema_alpha) * tick_factor\n",
    "\n",
    "#         # store the last tick\n",
    "#     prev_tick = this_tick\n",
    "\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    \n",
    "    df[\"tick_test\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(lambda x: 1 if x>0. else (-1. if x<0 else np.nan))\n",
    "    df.fillna(method=\"ffill\",inplace=True)\n",
    "    \n",
    "    df[\"tick_factor\"]=df[\"tick_test\"].ewm(span=20).mean()\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    \n",
    "    mysign = lambda x: 0 if abs(x)<1e-5 else (1 if x>0 else -1)\n",
    "    df[\"predict\"]=df[\"tick_factor\"].apply(mysign)\n",
    "    df[\"real_movement\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "    \n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    acc=np.mean(df[\"predict\"]==df[\"real_movement\"])\n",
    "    print(\"Accuracy of {}\".format(test_date),acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=pd.Timedelta(0)\n",
    "count=0\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    timestamp=pd.DataFrame({\"trade_time\":df.index})\n",
    "    dt=timestamp[\"trade_time\"].shift(-nforward)-timestamp[\"trade_time\"]\n",
    "    dt.dropna(axis=0,inplace=True)\n",
    "    dt.apply(lambda x:x.seconds).hist()\n",
    "    sum+=dt.sum()\n",
    "    count+=dt.shape[0]\n",
    "print(\"Average time interval between {} trades is\".format(nforward),sum/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
